{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.kb.KnowledgeBase' object has no attribute 'dump'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 127\u001b[0m\n\u001b[1;32m    123\u001b[0m         pickle\u001b[39m.\u001b[39mdump(test_dataset, f)\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 127\u001b[0m     create_kb()\n\u001b[1;32m    128\u001b[0m     \u001b[39m# train_el()\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m     \u001b[39m# nlp = spacy.load(output_dir / \"T-AIA-901-MPL_7/spacy/entity-linking\" / \"my_nlp_el\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39m# for ent in doc.ents:\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[39m#     print(ent.text, ent.label_, ent.kb_id_)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 50\u001b[0m, in \u001b[0;36mcreate_kb\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m probs \u001b[39m=\u001b[39m [\u001b[39m0.5\u001b[39m \u001b[39mfor\u001b[39;00m qid \u001b[39min\u001b[39;00m qids]\n\u001b[1;32m     48\u001b[0m kb\u001b[39m.\u001b[39madd_alias(alias\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParis\u001b[39m\u001b[39m\"\u001b[39m, entities\u001b[39m=\u001b[39mqids, probabilities\u001b[39m=\u001b[39mprobs)\n\u001b[0;32m---> 50\u001b[0m kb\u001b[39m.\u001b[39;49mdump(output_dir \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mT-AIA-901-MPL_7/spacy/entity-linking\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmy_kb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m nlp\u001b[39m.\u001b[39mto_disk(output_dir \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mT-AIA-901-MPL_7/spacy/entity-linking\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmy_nlp\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.kb.KnowledgeBase' object has no attribute 'dump'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.kb import KnowledgeBase\n",
    "from spacy.util import minibatch, compounding\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "\n",
    "output_dir = Path.cwd() / \"T-AIA-901-MPL_7\"\n",
    "\n",
    "def load_entities():\n",
    "    output_dir = Path.cwd()\n",
    "    entities_loc = output_dir / \"entities.csv\"\n",
    "\n",
    "    names = dict()\n",
    "    descriptions = dict()\n",
    "    with entities_loc.open(\"r\", encoding=\"utf8\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=\",\")\n",
    "        for row in csvreader:\n",
    "            qid = row[0]\n",
    "            name = row[1]\n",
    "            desc = row[2]\n",
    "            names[qid] = name\n",
    "            descriptions[qid] = desc\n",
    "    return names, descriptions\n",
    "\n",
    "def create_kb():\n",
    "    nlp =spacy.load(\"fr_core_news_sm\")\n",
    "    text = \"Je veux aller de Paris à Londres.\"\n",
    "    doc = nlp(text)\n",
    "    name_dict, desc_dict = load_entities()\n",
    "\n",
    "    kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=96)\n",
    "\n",
    "    for qid, desc in desc_dict.items():\n",
    "        desc_doc = nlp(desc)\n",
    "        desc_enc = desc_doc.vector\n",
    "        kb.add_entity(entity=qid, entity_vector=desc_enc, freq=342)\n",
    "    \n",
    "    for qid, name in name_dict.items():\n",
    "        kb.add_alias(alias=name, entities=[qid], probabilities=[1.0])\n",
    "    \n",
    "    qids = name_dict.keys()\n",
    "    probs = [0.5 for qid in qids]\n",
    "    kb.add_alias(alias=\"Paris\", entities=qids, probabilities=probs)\n",
    "\n",
    "    kb.dump(output_dir / \"T-AIA-901-MPL_7/spacy/entity-linking\" / \"my_kb\")\n",
    "    nlp.to_disk(output_dir / \"T-AIA-901-MPL_7/spacy/entity-linking\" / \"my_nlp\")\n",
    "\n",
    "\n",
    "def train_el():\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=1)\n",
    "    kb.load_bulk(output_dir / \"my_kb\")\n",
    "\n",
    "    dataset = []\n",
    "    csv_loc = output_dir / \"asset/SpeechDestination.csv\"\n",
    "    with csv_loc.open(\"r\", encoding=\"utf8\") as csvfile:\n",
    "        next(csvfile)\n",
    "        i = 0\n",
    "        for line in csvfile:\n",
    "            list = line.split(\",\")\n",
    "            list[3] = list[3][:-1]\n",
    "            if list[3] == \"false\":\n",
    "                text = list[0]\n",
    "                QIDDepart = \"DEPART\"\n",
    "                QIDDestination = \"DESTINATION\"\n",
    "                offsetDepart = (text.find(list[1]), text.find(list[1]) + len(list[1]))\n",
    "                offsetDestination = (text.find(list[2]), text.find(list[2]) + len(list[2]))\n",
    "                links_dict_depart = {QIDDepart: 1.0}\n",
    "                links_dict_destination = {QIDDestination: 1.0}\n",
    "                dataset.append((text, {\"links\": {offsetDepart: links_dict_depart, offsetDestination: links_dict_destination}}))\n",
    "                i += 1\n",
    "                if i == 100:\n",
    "                    break\n",
    "    \n",
    "    gold_ids = []\n",
    "    for text, annot in dataset:\n",
    "        for span, links_dict in annot[\"links\"].items():\n",
    "            for link, value in links_dict.items():\n",
    "                if value:\n",
    "                    gold_ids.append(link)\n",
    "    \n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    for i, (text, annot) in enumerate(dataset):\n",
    "        if i < len(dataset) * 0.8:\n",
    "            train_dataset.extend(dataset[i])\n",
    "        else:\n",
    "            test_dataset.extend(dataset[i])\n",
    "\n",
    "    TRAIN_DOCS = []\n",
    "    for text, annot in train_dataset:\n",
    "        doc = nlp(text)\n",
    "        TRAIN_DOCS.append(doc)\n",
    "    \n",
    "    entity_linker = nlp.create_pipe(\"entity_linker\", config={\"incl_prior\": False})\n",
    "    entity_linker.set_kb(kb)\n",
    "    nlp.add_pipe(entity_linker, last=True)\n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"entity_linker\"]\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(500):\n",
    "            random.shuffle(TRAIN_DOCS)\n",
    "            batches = minibatch(TRAIN_DOCS, size=compounding(4.0, 32.0, 1.001))\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
    "            if itn % 50 == 0:\n",
    "                print(itn, \"Losses\", losses)\n",
    "    print(itn, \"Losses\", losses)\n",
    "\n",
    "    nlp.to_disk(output_dir / \"T-AIA-901-MPL_7/spacy/entity-linking\" / \"my_nlp_el\")\n",
    "\n",
    "    with open(output_dir / \"T-AIA-901-MPL_7/spacy/entity-linking\" / \"test_set.plk\" , \"wb\") as f:\n",
    "        pickle.dump(test_dataset, f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_kb()\n",
    "    # train_el()\n",
    "\n",
    "    # nlp = spacy.load(output_dir / \"T-AIA-901-MPL_7/spacy/entity-linking\" / \"my_nlp_el\")\n",
    "    # text = 'Je vais de Paris à Marseille.'\n",
    "    # doc = nlp(text)\n",
    "    # for ent in doc.ents:\n",
    "    #     print(ent.text, ent.label_, ent.kb_id_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "back-P-6HUKwv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4c47255976128ec6dc18c5a6c6a38d19b3ecbc446285b5c4242f45c1511372b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
